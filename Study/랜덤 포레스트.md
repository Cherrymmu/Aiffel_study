#각자정리

[[지도학습]] - 랜덤 포레스트
# 파라미터
- n_estimators : 모델에서 사용할 트리 갯수(학습시 생성할 트리 갯수)
- criterion : 분할 품질을 측정하는 기능 (default : gini)
- max_depth : 트리의 최대 깊이
- min_samples_split : 내부 노드를 분할하는데 필요한 최소 샘플 수 (default : 2)
- min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1)
- min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율
- max_features : 각 노드에서 분할에 사용할 특징의 최대 수
- max_leaf_nodes : 리프 노드의 최대수
- min_impurity_decrease : 최소 불순도
- min_impurity_split : 나무 성장을 멈추기 위한 임계치
- bootstrap : 부트스트랩(중복허용 샘플링) 사용 여부
- oob_score : 일반화 정확도를 줄이기 위해 밖의 샘플 사용 여부
- n_jobs :적합성과 예측성을 위해 병렬로 실행할 작업 수
- random_state : 난수 seed 설정
- verbose : 실행 과정 출력 여부
- warm_start : 이전 호출의 솔루션을 재사용하여 합계에 더 많은 견적가를 추가
- class_weight : 클래스 가중치

# 특징
- 지도학습 알고리즘 (분류, 회귀)
	- 분류 : RandomForestClassifier
	- 회귀 : RandomForestRegression
- 의사결정나무의 앙상블
- 여러 개의 의사결정 트리로 구성
- 성능이 좋음 (과대적합 가능성 낮음)
- 부트스트랩 샘플링(데이터셋 중복 허용)
- 최종 다수결 투표
- 앙상블
	- 배깅 : 같은 알고리즘으로 여러 모델을 만들어 분류함 (랜덤포레스트)
	- 부스팅 : 학습과 예측을 하면서 가중치 반영 (XGboost)
## RandomForest Graph
![](https://i.imgur.com/g1E4lbt.png)


# 장점과 단점
장점
- 일반화 및 성능이 우수하다.
- 파라미터 조정이 용이 하다.
- 데이터 scale 변환이 불필요하다.
- 과대적합이 잘 되지 않는다.
- 분류 및 회귀 문제에 모두 사용이 가능하다.
- 결측치를 다루기 쉽다.
- 특성 중요도를 구할 수 있다.
- 대용량 데이터 처리가 쉽다.
단점
- 개별 트리 분석이 어렵고 트리 분리가 복잡해지는 경향이 존재한다. ( 해석이 어려움 )
- 차원이 크고 희소한 데이터는 성능이 미흡하다.
- 훈련시에 메모리 소모가 크다.
- Train data를 추가해도 모델 성능 개선이 어렵다.
- 변수가 너무 적은 경우 랜덤성이 부족하여 성능이 떨어짐