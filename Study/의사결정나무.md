#각자정리


[[지도학습]] - 의사결정나무

# 파라미터
- criterion : 분할 품질을 측정하는 기능 (default : gini)
- splitter : 각 노드에서 분할을 선택하는 데 사용되는 전략 (default : best)
- max_depth : 트리의 최대 깊이 (값이 클수록 모델의 복잡도가 올라간다.)
- min_samples_split : 자식 노드를 분할하는데 필요한 최소 샘플 수 (default : 2)
- min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1)
- min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율
- max_features : 각 노드에서 분할에 사용할 특징의 최대 수
- random_state : 난수 seed 설정
- max_leaf_nodes : 리프 노드의 최대수
- min_impurity_decrease : 최소 불순도
- min_impurity_split : 나무 성장을 멈추기 위한 임계치
- class_weight : 클래스 가중치
- presort : 데이터 정렬 필요 여부
# 특징
- 지도학습 알고리즘(분류, 회귀)
- 직관적인 알고리즘(이해 쉬움)
- 과대적합되기 쉬운 알고리즘(트리 깊이 제한 필요)
- 정보이득(infomation gain)이 최대가 되는 특성을 나누는 기준(불순도를 측정하는 기준)은 '지니'와 '엔트로피'가 사용됨
- 데이터가 한 종류만 있다면 엔트로피/지니 불순도는 0에 가까움, 서로 다른 데이터의 비율이 비슷하면 1에 가까움
- 정보이득(infomation gain)이 최대(-1  불순도)
- 맨위의 마디를 부리 노드라 하며, 이는 모든 분류 대상이 되는 모든 자료 집단을 포함한다.
- 상위 마디를 부모 마디, 하위 마디를 자식 마디라 하며, 더이상 분기되지 않는 마디를 최종노드(terminal node)라고 부른다.
- 가지 분할은 나무의 가지를 생성하는 과정을 말하고, 가지치기(pruning)는 생성된 가지를 잘라내어서, 모형을 단순화 하는 과정을 말한다.
- 분기가 거듭될수록 그에 해당하는 데이터의 개수는 줄어든다.
- 상위 노드로부터, 하위 노드로 트리구조를 형성하는 모든 단계마다 기준값의 선택이 중요하다.
- 목표 변수가 이산형인 경우 분류나무, 목표 변수가 연속형인 경우에는 회귀 나무로 구분된다.
## DesicionTree Graph
![](https://i.imgur.com/Unyawqx.png)

# 장점과 단점
장점
- 모델의 해석이 쉽고 직관적이다.
- 범주와 연속형 수치를 모두 예측할 수 있다.
- 구조가 단순하여 해석이 용이하고, 유용한 입력 변수의 파악, 예측 변수 간의 상호작용, 비 선형성을 고려하여 수학적 가정이 불필요한 비모수적 모형이다.
- 데이터를 분할할 때 스케일의 영향을 받지 않으므로 Feature의 정규화나 표준화 같은 전처리 과정이 필요 없다.
- 시장조사, 광고조사, 의학연구, 품질관리 등 다양한 분야에서 활용되고 있다.
- 고객 타겟팅, 고객의 신용점수화, 캠페인에 대한 반응, 고객 행동 예측 등에 유용하다.
단점
- 다수결의 법칙 또는 평균에 의해 예측하기 때문에 이상치가 있을 경우 예측력이 떨어진다.
- 분류 기준값의 경계선 근방의 자료  값에 대해서는 오차가 클수 있다.
- 로지스틱 회귀와 같이 각 예측 변수의 효과를 파악하기 어렵다.
- 새로운 자료에 대한 예측이 불안정 할 수 있다.
- 상위 노드로부터, 하위 노드로 트리구조를 형성하는 모든 단게마다 기준값의 선택이 중요하다.
- 시계열 분석에 사용하기에는 어렵다.
- 비사각 영역에서는 예측 문제가 있다. 그러므로 선형 예측 데이터는 어렵다.

